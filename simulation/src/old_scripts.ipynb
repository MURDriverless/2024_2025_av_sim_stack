{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colision(self, x, y):\n",
    "        for cone in self.car.forward_cones:\n",
    "            ex = cone.x - x; ey = cone.y - y\n",
    "            if ex**2 + ey**2 - cone.radius**2 <= 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def sense_obstacle(self):\n",
    "        data = []\n",
    "        points = []\n",
    "        x = self.car.get_position()[0] + self.pos[0]\n",
    "        y = self.car.get_position()[1] + self.pos[1]\n",
    "        for angle in np.linspace(self.angle_min + self.car.state.yaw, self.angle_max + self.car.state.yaw,self.angle_num, True):\n",
    "            sense = False\n",
    "            x1 = x + self.range_min*math.cos(angle)\n",
    "            y1 = y + self.range_min*math.sin(angle)\n",
    "\n",
    "            x2 = x + self.range_max*math.cos(angle)\n",
    "            y2 = y + self.range_max*math.sin(angle)\n",
    "\n",
    "            for i in range(self.range_num+1):\n",
    "                u = i/self.range_num\n",
    "                x3 = x2*u + x1*(1 - u)\n",
    "                y3 = y2*u + y1*(1 - u)\n",
    "                if self.is_colision(x3, y3):\n",
    "                    dis = self.distance((x3, y3))\n",
    "                    data.append({'position': (x3, y3), 'distance': dis, 'angle': angle})\n",
    "                    points.append((x3, y3))\n",
    "                    sense = True\n",
    "                    break\n",
    "            if not sense:\n",
    "                data.append({'position': None, 'distance': self.range_max, 'angle': angle})\n",
    "        self.sense_data = data\n",
    "        self.points = points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sense_obstacle_fast_angles(self, track):\n",
    "\n",
    "        data = []\n",
    "        points = []\n",
    "\n",
    "        for angle in np.linspace(self.angle_min, self.angle_max, self.angle_num, True):\n",
    "            direction = np.array([math.cos(angle), math.sin(angle)])\n",
    "            closest_point = None\n",
    "            min_dist = self.range_max\n",
    "\n",
    "            for cone in self.car.forward_cones_lidar:\n",
    "                circle_center = np.array([cone.x, cone.y])\n",
    "                r = cone.radius\n",
    "                hit = self.ray_circle_intersection(self.pos, direction, circle_center, r)\n",
    "                if hit is not None:\n",
    "                    dist = np.linalg.norm(hit - self.pos)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        closest_point = hit\n",
    "                        color = cone.color\n",
    "\n",
    "            if closest_point is not None:\n",
    "                data.append({'position': tuple(closest_point), 'distance': min_dist, 'angle': angle, 'color': color})\n",
    "                points.append(tuple(closest_point))\n",
    "            else:\n",
    "                data.append({'position': None, 'distance': self.range_max, 'angle': angle, 'color': None})\n",
    "\n",
    "        self.sense_data = data\n",
    "        self.points = points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_visible_cones(self):\n",
    "\n",
    "        visible_cones = []\n",
    "\n",
    "        forward_vec = np.array([np.cos(self.sense_yaw), np.sin(self.sense_yaw)])\n",
    "\n",
    "        # Step 2: Check distance to each cone\n",
    "        for cone in self.car.track.cones:\n",
    "\n",
    "            dx = cone.x - self.car.state.x\n",
    "            dy = cone.y - self.car.state.y\n",
    "            dist = math.hypot(dx, dy)\n",
    "\n",
    "            direct_to_cone = np.array([dx,dy])/dist\n",
    "            dot_product = np.clip(np.dot(forward_vec, direct_to_cone), -1.0, 1.0)\n",
    "            angle_deg = np.degrees(np.arccos(dot_product))\n",
    "\n",
    "\n",
    "            if dist <= self.range_max and angle_deg <= 85:\n",
    "                visible_cones.append(cone)\n",
    "                self.seen_cones.add(cone)\n",
    "\n",
    "        self.visible_cones = visible_cones\n",
    "\n",
    "    def distance(self, obs_pose):\n",
    "        ex = obs_pose[0] - self.pos[0]\n",
    "        ey = obs_pose[1] - self.pos[1]\n",
    "        return math.hypot(ex, ey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_euclidean_clustering(self, distance_threshold=0.5, min_cluster_size=1):\n",
    "        points = self.points\n",
    "        tree = cKDTree(points)\n",
    "        visited = np.zeros(len(points), dtype=bool)\n",
    "        clusters = []\n",
    "\n",
    "        for i in range(len(points)):\n",
    "            if visited[i]:\n",
    "                continue\n",
    "\n",
    "            queue = [i]\n",
    "            cluster = []\n",
    "\n",
    "            while queue:\n",
    "                idx = queue.pop()\n",
    "                if visited[idx]:\n",
    "                    continue\n",
    "                visited[idx] = True\n",
    "                cluster.append(idx)\n",
    "                neighbors = tree.query_ball_point(points[idx], distance_threshold)\n",
    "                queue.extend([n for n in neighbors if not visited[n]])\n",
    "\n",
    "            if len(cluster) >= min_cluster_size:\n",
    "                clusters.append(cluster)\n",
    "\n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_detections(self, clusters, subplot_index=111):\n",
    "        # Map positions to indices in self.sense_data\n",
    "        pos_to_data_index = {\n",
    "            tuple(d['position']): i\n",
    "            for i, d in enumerate(self.sense_data)\n",
    "            if d['position'] is not None\n",
    "        }\n",
    "\n",
    "        # Generate a color for each cluster\n",
    "        colors = cm.rainbow(np.linspace(0, 1, len(clusters)))\n",
    "\n",
    "        ax = plt.subplot(subplot_index, polar=True)\n",
    "\n",
    "        for cluster, color in zip(clusters, colors):\n",
    "            cluster_angles = []\n",
    "            cluster_distances = []\n",
    "            for idx in cluster:\n",
    "                pos = self.points[idx]\n",
    "                data_idx = pos_to_data_index.get(tuple(pos))\n",
    "                if data_idx is not None:\n",
    "                    d = self.sense_data[data_idx]\n",
    "                    cluster_angles.append(d['angle'])\n",
    "                    cluster_distances.append(d['distance'])\n",
    "\n",
    "            ax.scatter(cluster_angles, cluster_distances, s=10, color=color,\n",
    "                    label=f'Cluster {clusters.index(cluster)}')\n",
    "\n",
    "        ax.set_title(\"Clustered LiDAR Scan (Polar)\")\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def estimate_cone_center(self, cluster_indices, known_radius=0.1):\n",
    "        cluster = np.array([self.points[i] for i in cluster_indices])\n",
    "        n = len(cluster)\n",
    "\n",
    "        if n == 0:\n",
    "            return None\n",
    "\n",
    "        elif n == 1:\n",
    "            idx = cluster_indices[0]\n",
    "            point = self.points[idx]\n",
    "            sense_idx = self.find_matching_sense_idx(point)\n",
    "            sense = self.sense_data[sense_idx]\n",
    "            x, y = sense['position']\n",
    "            angle = sense['angle']\n",
    "            dx = math.cos(angle)\n",
    "            dy = math.sin(angle)\n",
    "            return (x - known_radius * dx, y - known_radius * dy)\n",
    "\n",
    "        elif n == 2:\n",
    "            p1, p2 = cluster\n",
    "            midpoint = (p1 + p2) / 2\n",
    "            vec = p2 - p1\n",
    "            norm_vec = np.array([-vec[1], vec[0]])  # perpendicular\n",
    "            norm_vec = norm_vec / np.linalg.norm(norm_vec)\n",
    "            center1 = midpoint + norm_vec * known_radius\n",
    "            center2 = midpoint - norm_vec * known_radius\n",
    "            return tuple(center1 if np.linalg.norm(center1) < np.linalg.norm(center2) else center2)\n",
    "\n",
    "        else:\n",
    "            first = complex(*cluster[0])\n",
    "            middle = complex(*cluster[len(cluster) // 2])\n",
    "            last = complex(*cluster[-1])\n",
    "            pos = self.circle_from_3_points(first, middle, last) # pos, r\n",
    "            return (pos.real, pos.imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_cones(self):\n",
    "        \"\"\"\n",
    "        Converts self.cone_pos and cluster color info into a list of Cone objects.\n",
    "        \"\"\"\n",
    "        detected_cones = []\n",
    "\n",
    "        # Build a mapping from point (x, y) → index in sense_data\n",
    "        pos_to_index = {\n",
    "            tuple(d['position']): i\n",
    "            for i, d in enumerate(self.sense_data)\n",
    "            if d['position'] is not None\n",
    "        }\n",
    "\n",
    "        if self.clusters and self.cone_pos:\n",
    "            for cluster, pos in zip(self.clusters, self.cone_pos):\n",
    "                if pos is None:\n",
    "                    continue\n",
    "\n",
    "                # Count dominant color\n",
    "                color_counts = Counter()\n",
    "                for idx in cluster:\n",
    "                    point = tuple(self.points[idx])\n",
    "                    sense_idx = pos_to_index.get(point)\n",
    "                    if sense_idx is not None:\n",
    "                        color = self.sense_data[sense_idx].get('color', 'black')\n",
    "                        color_counts[color] += 1\n",
    "\n",
    "                dominant_color = color_counts.most_common(1)[0][0] if color_counts else 'black'\n",
    "                cone = Cone(pos[0], pos[1], dominant_color)\n",
    "                detected_cones.append(cone)\n",
    "\n",
    "        return detected_cones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lidar(self):\n",
    "        plt.figure(figsize=(12, 6))  # wider to fit two plots\n",
    "\n",
    "        # --- Subplot 1: Cartesian LiDAR View ---\n",
    "        ax1 = plt.subplot(1, 2, 1)\n",
    "        for i in range(len(self.sense_data)):\n",
    "            angle = self.sense_angle_min + self.resolution * i\n",
    "            x = self.pos[0]\n",
    "            y = self.pos[1]\n",
    "            rayx = [x, x + math.cos(angle) * self.sense_data[i]['distance']]\n",
    "            rayy = [y, y + math.sin(angle) * self.sense_data[i]['distance']]\n",
    "            ax1.plot(rayx, rayy, '-b', linewidth=0.5)\n",
    "\n",
    "\n",
    "        # Build a mapping from point (x, y) → index in sense_data\n",
    "        pos_to_index = {\n",
    "            tuple(d['position']): i\n",
    "            for i, d in enumerate(self.sense_data)\n",
    "            if d['position'] is not None\n",
    "        }\n",
    "\n",
    "        # Plot cone centers with dominant cluster color\n",
    "        if self.clusters and self.cone_pos:\n",
    "            for cluster, pos in zip(self.clusters, self.cone_pos):\n",
    "                if pos is None:\n",
    "                    continue\n",
    "\n",
    "                # Use original sense_data indices to get colors\n",
    "                color_counts = Counter()\n",
    "                for idx in cluster:\n",
    "                    point = tuple(self.points[idx])\n",
    "                    sense_idx = pos_to_index.get(point)\n",
    "                    if sense_idx is not None:\n",
    "                        color = self.sense_data[sense_idx].get('color', 'black')\n",
    "                        color_counts[color] += 1\n",
    "\n",
    "                dominant_color = color_counts.most_common(1)[0][0] if color_counts else 'black'\n",
    "                circle = plt.Circle(pos, 0.1, color=dominant_color, fill=True)\n",
    "                ax1.add_patch(circle)\n",
    "\n",
    "        if self.car:\n",
    "            ax1.scatter(self.car.state.x, self.car.state.y, c='purple', marker='s', label=\"Car\")\n",
    "\n",
    "        if self.car:\n",
    "            ax1.scatter(self.pos[0], self.pos[1], c='cyan', marker='s', label=\"Lidar\")\n",
    "\n",
    "        ax1.set_title(\"LiDAR Vision (Cartesian)\")\n",
    "        ax1.set_aspect('equal')\n",
    "        ax1.legend()\n",
    "\n",
    "        # --- Subplot 2: Polar Cluster View ---\n",
    "        self.plot_detections(self.clusters, subplot_index=122)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sense_obstacle_fast_cones(self, track):\n",
    "\n",
    "        data = []\n",
    "        visible_cones = []\n",
    "\n",
    "        angles = np.linspace(self.sense_angle_min, self.sense_angle_max, self.angle_num, True)\n",
    "        \n",
    "\n",
    "        for angle in angles:\n",
    "            data.append({\n",
    "                'position': None,\n",
    "                'distance': self.range_max,\n",
    "                'angle': angle,\n",
    "                'color': None\n",
    "            })\n",
    "\n",
    "        for cone in track.cones:\n",
    "\n",
    "            cone = \n",
    "            \n",
    "            vec_to_cone = np.array([cone.x - self.pos[0], cone.y - self.pos[1]])\n",
    "            dist = np.linalg.norm(vec_to_cone)\n",
    "\n",
    "            if dist > self.range_max:\n",
    "                continue\n",
    "            \n",
    "            angle_to_cone = np.arctan2(vec_to_cone[1], vec_to_cone[0])\n",
    "            relative_angle = angle_to_cone - self.sense_yaw\n",
    "            relative_angle = np.arctan2(np.sin(relative_angle), np.cos(relative_angle))\n",
    "\n",
    "            if (self.angle_max < relative_angle) or (relative_angle < self.angle_min):\n",
    "                continue\n",
    "            \n",
    "            #visible_cones.append(cone)\n",
    "            #self.seen_cones.add(cone)\n",
    "\n",
    "            angular_radius = np.arcsin(cone.radius / dist)\n",
    "            lower_bound = relative_angle - angular_radius\n",
    "            upper_bound = relative_angle + angular_radius\n",
    "\n",
    "            candidate_indices = np.where((angles-self.sense_yaw >= lower_bound) & (angles-self.sense_yaw <= upper_bound))[0]\n",
    "\n",
    "            for idx in candidate_indices:\n",
    "                ray_angle = angles[idx]\n",
    "                ray_dir = np.array([np.cos(ray_angle), np.sin(ray_angle)])\n",
    "\n",
    "                hit = self.ray_circle_intersection(self.pos, ray_dir, np.array([cone.x, cone.y]), cone.radius)\n",
    "                \n",
    "                if hit is not None:\n",
    "                    hit_dist = np.linalg.norm(hit - self.pos)\n",
    "                    if hit_dist < data[idx]['distance']:\n",
    "                        # Update the stored data immediately\n",
    "                        data[idx]['distance'] = hit_dist\n",
    "                        data[idx]['position'] = tuple(hit)\n",
    "                        data[idx]['color'] = cone.color\n",
    "\n",
    "        self.sense_data = data\n",
    "\n",
    "        # Convert sensed points to car frame\n",
    "        self.relative_sense_data = []\n",
    "        for d in data:\n",
    "            if d['position'] is None:\n",
    "                self.relative_sense_data.append({\n",
    "                    'position': None,\n",
    "                    'distance': self.range_max,\n",
    "                    'angle': d['angle']-self.sense_yaw,\n",
    "                    'color': None\n",
    "                })\n",
    "                continue\n",
    "            px, py = d['position']\n",
    "            dx = px - self.pos[0]\n",
    "            dy = py - self.pos[1]\n",
    "\n",
    "            # Rotate by -yaw (from world to car frame)\n",
    "            cos_yaw = np.cos(-self.sense_yaw)\n",
    "            sin_yaw = np.sin(-self.sense_yaw)\n",
    "            rel_x = cos_yaw * dx - sin_yaw * dy\n",
    "            rel_y = sin_yaw * dx + cos_yaw * dy\n",
    "\n",
    "            self.relative_sense_data.append({\n",
    "                'position': (rel_x, rel_y),\n",
    "                'distance': d['distance'],\n",
    "                'angle': d['angle']-self.sense_yaw,\n",
    "                'color': d['color']\n",
    "            })\n",
    "\n",
    "        self.points = {\n",
    "        'positions': [d['position'] for i, d in enumerate(self.relative_sense_data) if d['position'] is not None],\n",
    "        'indices': [i for i, d in enumerate(self.relative_sense_data) if d['position'] is not None]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def match_cones(self, max_dist=0.3):\n",
    "    #     raw_matches = []\n",
    "    #     distances = []\n",
    "\n",
    "    #     for i, c_curr in enumerate(self.lidar.relative_cones):\n",
    "    #         best_match = None\n",
    "    #         min_dist = float('inf')\n",
    "    #         for j, c_prev in enumerate(self.prev_lidar_data):\n",
    "    #             if c_curr.color != c_prev.color:\n",
    "    #                 continue\n",
    "    #             dist = np.hypot(c_curr.x - c_prev.x, c_curr.y - c_prev.y)\n",
    "    #             if dist < min_dist and dist < max_dist:\n",
    "    #                 best_match = j\n",
    "    #                 min_dist = dist\n",
    "    #         if best_match is not None:\n",
    "    #             raw_matches.append((best_match, i))  # (prev_idx, curr_idx)\n",
    "    #             distances.append(min_dist)\n",
    "\n",
    "    #     # Filter using 2 standard deviations above the mean\n",
    "    #     if distances:\n",
    "    #         mean = np.mean(distances)\n",
    "    #         std_dev = np.std(distances)\n",
    "    #         threshold = mean + 2 * std_dev\n",
    "    #         filtered_matches = [\n",
    "    #             match for match, dist in zip(raw_matches, distances) if dist <= threshold\n",
    "    #         ]\n",
    "    #     else:\n",
    "    #         filtered_matches = []\n",
    "\n",
    "    #     return filtered_matches\n",
    "\n",
    "\n",
    "    # def match_lidar_points(self, max_dist=0.3):\n",
    "    #     raw_matches = []\n",
    "    #     distances = []\n",
    "\n",
    "    #     for i, curr in enumerate(self.lidar.relative_sense_data):  # current frame\n",
    "    #         best_match = None\n",
    "    #         min_dist = float('inf')\n",
    "    #         curr_x, curr_y = curr['position']\n",
    "\n",
    "    #         for j, prev in enumerate(self.prev_lidar_data):  # previous frame\n",
    "    #             prev_x, prev_y = prev['position']\n",
    "    #             dist = np.hypot(curr_x - prev_x, curr_y - prev_y)\n",
    "    #             if dist < min_dist and dist < max_dist:\n",
    "    #                 best_match = j\n",
    "    #                 min_dist = dist\n",
    "\n",
    "    #         if best_match is not None:\n",
    "    #             raw_matches.append((best_match, i))  # (prev_idx, curr_idx)\n",
    "    #             distances.append(min_dist)\n",
    "\n",
    "    #     # Filter using mean + 2 std dev\n",
    "    #     if distances:\n",
    "    #         mean = np.mean(distances)\n",
    "    #         std_dev = np.std(distances)\n",
    "    #         threshold = mean + 2 * std_dev\n",
    "    #         filtered_matches = [\n",
    "    #             match for match, dist in zip(raw_matches, distances) if dist <= threshold\n",
    "    #         ]\n",
    "    #     else:\n",
    "    #         filtered_matches = []\n",
    "\n",
    "    #     return filtered_matches\n",
    "\n",
    "    \n",
    "\n",
    "    # def estimate_transform(self, matches):\n",
    "    #     if len(matches) < 2:\n",
    "    #         return None\n",
    "\n",
    "    #     prev_pts = np.array([[self.prev_lidar_data[i].x, self.prev_lidar_data[i].y] for i, _ in matches])\n",
    "    #     curr_pts = np.array([[self.lidar.relative_cones[j].x, self.lidar.relative_cones[j].y] for _, j in matches])\n",
    "\n",
    "    #     centroid_prev = np.mean(prev_pts, axis=0)\n",
    "    #     centroid_curr = np.mean(curr_pts, axis=0)\n",
    "\n",
    "    #     P = prev_pts - centroid_prev\n",
    "    #     C = curr_pts - centroid_curr\n",
    "\n",
    "    #     H = C.T @ P\n",
    "    #     U, _, Vt = np.linalg.svd(H)\n",
    "    #     R = Vt.T @ U.T\n",
    "\n",
    "    #     if np.linalg.det(R) < 0:\n",
    "    #         Vt[1, :] *= -1\n",
    "    #         R = Vt.T @ U.T\n",
    "\n",
    "    #     t = centroid_prev - R @ centroid_curr\n",
    "    #     yaw = np.arctan2(R[1, 0], R[0, 0])\n",
    "\n",
    "    #     return t[0], t[1], yaw\n",
    "\n",
    "    # def estimate_transform(self, matches):\n",
    "    #     if len(matches) < 2:\n",
    "    #         return None\n",
    "\n",
    "    #     # Extract matched points\n",
    "    #     prev_pts = np.array([\n",
    "    #         self.prev_lidar_data[i]['position'] for i, _ in matches\n",
    "    #     ])\n",
    "    #     curr_pts = np.array([\n",
    "    #         self.lidar.relative_sense_data[j]['position'] for _, j in matches\n",
    "    #     ])\n",
    "\n",
    "    #     centroid_prev = np.mean(prev_pts, axis=0)\n",
    "    #     centroid_curr = np.mean(curr_pts, axis=0)\n",
    "\n",
    "    #     P = prev_pts - centroid_prev\n",
    "    #     C = curr_pts - centroid_curr\n",
    "\n",
    "    #     H = C.T @ P\n",
    "    #     U, _, Vt = np.linalg.svd(H)\n",
    "    #     R = Vt.T @ U.T\n",
    "\n",
    "    #     if np.linalg.det(R) < 0:\n",
    "    #         Vt[1, :] *= -1\n",
    "    #         R = Vt.T @ U.T\n",
    "\n",
    "    #     t = centroid_prev - R @ centroid_curr\n",
    "    #     yaw = np.arctan2(R[1, 0], R[0, 0])\n",
    "\n",
    "    #     return t[0], t[1], yaw\n",
    "\n",
    "\n",
    "\n",
    "    # def update_pose(self, dx, dy, dyaw):\n",
    "    #     R_global = np.array([\n",
    "    #         [np.cos(self.state_e.yaw), -np.sin(self.state_e.yaw)],\n",
    "    #         [np.sin(self.state_e.yaw),  np.cos(self.state_e.yaw)]\n",
    "    #     ])\n",
    "    #     delta_global = R_global @ np.array([dx, dy])\n",
    "    #     x_new = self.state_e.x + delta_global[0]\n",
    "    #     y_new = self.state_e.y + delta_global[1]\n",
    "    #     yaw_new = self.state_e.yaw + dyaw\n",
    "\n",
    "    #     self.state_e.x = x_new\n",
    "    #     self.state_e.y = y_new\n",
    "    #     self.state_e.yaw = yaw_new\n",
    "\n",
    "\n",
    "    # def extract_transform(self, transformation):\n",
    "    #     \"\"\"\n",
    "    #     Extract the translation (dx, dy) and yaw (dyaw) from the 3x3 transformation matrix.\n",
    "    #     \"\"\"\n",
    "    #     # Extract rotation and translation\n",
    "    #     R = transformation[:2, :2]\n",
    "    #     t = transformation[:2, 2]\n",
    "\n",
    "    #     # Compute yaw from the rotation matrix\n",
    "    #     yaw = np.arctan2(R[1, 0], R[0, 0])\n",
    "\n",
    "    #     # dx, dy are the translation components\n",
    "    #     dx, dy = t[0], t[1]\n",
    "\n",
    "    #     return dx, dy, yaw\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # def estimate_transform_icp(self, prev_pts, curr_pts, max_iterations=50, threshold=1e-6):\n",
    "    #     \"\"\"\n",
    "    #     Perform 2D ICP (Iterative Closest Point) to estimate the transform between two point clouds.\n",
    "    #     \"\"\"\n",
    "    #     # Start with identity transformation\n",
    "    #     transformation = np.eye(3)\n",
    "        \n",
    "    #     for iteration in range(max_iterations):\n",
    "    #         # Find closest points in current to previous\n",
    "    #         closest_points = self.find_closest_points(prev_pts, curr_pts)\n",
    "            \n",
    "    #         # Estimate the rigid transformation (rotation and translation)\n",
    "    #         R, t = self.compute_rigid_transform(prev_pts, closest_points)\n",
    "            \n",
    "    #         # Apply the transformation to the current points\n",
    "    #         curr_pts = self.apply_transformation_to_points(curr_pts, R, t)\n",
    "            \n",
    "    #         # Compute the new transformation matrix\n",
    "    #         transformation_new = np.eye(3)\n",
    "    #         transformation_new[:2, :2] = R\n",
    "    #         transformation_new[:2, 2] = t\n",
    "            \n",
    "    #         # Check for convergence\n",
    "    #         if np.linalg.norm(transformation_new - transformation) < threshold:\n",
    "    #             break\n",
    "                \n",
    "    #         transformation = transformation_new\n",
    "\n",
    "    #     return transformation  # 3x3 transformation matrix (rotation + translation)\n",
    "\n",
    "    # def find_closest_points(self, prev_pts, curr_pts):\n",
    "    #     \"\"\"\n",
    "    #     Find the closest points in prev_pts for each point in curr_pts.\n",
    "    #     \"\"\"\n",
    "    #     closest_points = []\n",
    "    #     for curr_point in curr_pts:\n",
    "    #         distances = np.linalg.norm(prev_pts - curr_point, axis=1)\n",
    "    #         closest_idx = np.argmin(distances)\n",
    "    #         closest_points.append(prev_pts[closest_idx])\n",
    "    #     return np.array(closest_points)\n",
    "    \n",
    "    # def compute_rigid_transform(self, A, B):\n",
    "    #     \"\"\"\n",
    "    #     Compute the rigid transform (rotation and translation) that aligns A to B in 2D.\n",
    "    #     \"\"\"\n",
    "    #     centroid_A = np.mean(A, axis=0)\n",
    "    #     centroid_B = np.mean(B, axis=0)\n",
    "        \n",
    "    #     A_centered = A - centroid_A\n",
    "    #     B_centered = B - centroid_B\n",
    "        \n",
    "    #     H = A_centered.T @ B_centered\n",
    "    #     U, _, Vt = np.linalg.svd(H)\n",
    "    #     R = Vt.T @ U.T\n",
    "        \n",
    "    #     if np.linalg.det(R) < 0:\n",
    "    #         Vt[1, :] *= -1\n",
    "    #         R = Vt.T @ U.T\n",
    "        \n",
    "    #     t = centroid_B - R @ centroid_A\n",
    "        \n",
    "    #     return R, t\n",
    "\n",
    "\n",
    "    # def apply_transformation_to_points(self, points, R, t):\n",
    "    #     \"\"\"\n",
    "    #     Apply the transformation (rotation and translation) to a set of points.\n",
    "    #     \"\"\"\n",
    "    #     return np.dot(points, R.T) + t\n",
    "\n",
    "\n",
    "    # def apply_transformation(self, point, transform):\n",
    "    #     \"\"\"\n",
    "    #     Apply the transformation to a single point.\n",
    "    #     \"\"\"\n",
    "    #     point_homogeneous = np.append(point, 1)  # Convert to homogeneous coordinates\n",
    "    #     transformed_point = transform @ point_homogeneous\n",
    "    #     return transformed_point[:2]  # Return only x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def match_lidar_points(self, max_dist=0.3, max_iterations=50, threshold=1e-6):\n",
    "    #     \"\"\"\n",
    "    #     Match lidar points using ICP method and return matched points.\n",
    "    #     \"\"\"\n",
    "    #     raw_matches = []\n",
    "    #     distances = []\n",
    "\n",
    "    #     # Get the previous and current lidar data\n",
    "    #     prev_points = np.array([cone['position'] for cone in self.prev_lidar_data])  # previous frame\n",
    "    #     curr_points = np.array([cone['position'] for cone in self.lidar.relative_sense_data])  # current frame\n",
    "        \n",
    "    #     # Perform ICP to estimate the transformation\n",
    "    #     transform = self.estimate_transform_icp(prev_points, curr_points, max_iterations, threshold)\n",
    "\n",
    "    #     if transform is not None:\n",
    "    #         # The transformation (dx, dy, dyaw) is embedded in this transformation\n",
    "    #         dx, dy, dyaw = self.extract_transform(transform)\n",
    "\n",
    "    #         # Now apply the transformation to the pose\n",
    "    #         self.update_pose(dx, dy, dyaw)\n",
    "\n",
    "    #         return raw_matches  # Matching is now done, and the pose is updated.\n",
    "\n",
    "    #     return []  # Return empty if transformation is not valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def point_based_matching(self, point_pairs):\n",
    "    #     \"\"\"\n",
    "    #     This function is based on the paper \"Robot Pose Estimation in Unknown Environments by Matching 2D Range Scans\"\n",
    "    #     by F. Lu and E. Milios.\n",
    "\n",
    "    #     :param point_pairs: the matched point pairs [((x1, y1), (x1', y1')), ..., ((xi, yi), (xi', yi')), ...]\n",
    "    #     :return: the rotation angle and the 2D translation (x, y) to be applied for matching the given pairs of points\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     x_mean = 0\n",
    "    #     y_mean = 0\n",
    "    #     xp_mean = 0\n",
    "    #     yp_mean = 0\n",
    "    #     n = len(point_pairs)\n",
    "\n",
    "    #     if n == 0:\n",
    "    #         return None, None, None\n",
    "\n",
    "    #     for pair in point_pairs:\n",
    "\n",
    "    #         (x, y), (xp, yp) = pair\n",
    "\n",
    "    #         x_mean += x\n",
    "    #         y_mean += y\n",
    "    #         xp_mean += xp\n",
    "    #         yp_mean += yp\n",
    "\n",
    "    #     x_mean /= n\n",
    "    #     y_mean /= n\n",
    "    #     xp_mean /= n\n",
    "    #     yp_mean /= n\n",
    "\n",
    "    #     s_x_xp = 0\n",
    "    #     s_y_yp = 0\n",
    "    #     s_x_yp = 0\n",
    "    #     s_y_xp = 0\n",
    "    #     for pair in point_pairs:\n",
    "\n",
    "    #         (x, y), (xp, yp) = pair\n",
    "\n",
    "    #         s_x_xp += (x - x_mean)*(xp - xp_mean)\n",
    "    #         s_y_yp += (y - y_mean)*(yp - yp_mean)\n",
    "    #         s_x_yp += (x - x_mean)*(yp - yp_mean)\n",
    "    #         s_y_xp += (y - y_mean)*(xp - xp_mean)\n",
    "\n",
    "    #     rot_angle = math.atan2(s_x_yp - s_y_xp, s_x_xp + s_y_yp)\n",
    "    #     translation_x = xp_mean - (x_mean*math.cos(rot_angle) - y_mean*math.sin(rot_angle))\n",
    "    #     translation_y = yp_mean - (x_mean*math.sin(rot_angle) + y_mean*math.cos(rot_angle))\n",
    "\n",
    "    #     return rot_angle, translation_x, translation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ICP parameters\n",
    "EPS = 0.0001\n",
    "MAX_ITER = 100\n",
    "\n",
    "show_animation = False\n",
    "\n",
    "\n",
    "def icp_matching(previous_points, current_points):\n",
    "    \"\"\"\n",
    "    Iterative Closest Point matching\n",
    "    - input\n",
    "    previous_points: 2D or 3D points in the previous frame\n",
    "    current_points: 2D or 3D points in the current frame\n",
    "    - output\n",
    "    R: Rotation matrix\n",
    "    T: Translation vector\n",
    "    \"\"\"\n",
    "    H = None  # homogeneous transformation matrix\n",
    "\n",
    "    dError = np.inf\n",
    "    preError = np.inf\n",
    "    count = 0\n",
    "\n",
    "    if show_animation:\n",
    "        fig = plt.figure()\n",
    "        if previous_points.shape[0] == 3:\n",
    "           fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    while dError >= EPS:\n",
    "        count += 1\n",
    "\n",
    "        if show_animation:  # pragma: no cover\n",
    "            plot_points(previous_points, current_points, fig)\n",
    "            plt.pause(0.1)\n",
    "\n",
    "        indexes, error = nearest_neighbor_association(previous_points, current_points)\n",
    "        Rt, Tt = svd_motion_estimation(previous_points[:, indexes], current_points)\n",
    "        # update current points\n",
    "        current_points = (Rt @ current_points) + Tt[:, np.newaxis]\n",
    "\n",
    "        dError = preError - error\n",
    "        print(\"Residual:\", error)\n",
    "\n",
    "        if dError < 0:  # prevent matrix H changing, exit loop\n",
    "            print(\"Not Converge...\", preError, dError, count)\n",
    "            break\n",
    "\n",
    "        preError = error\n",
    "        H = update_homogeneous_matrix(H, Rt, Tt)\n",
    "\n",
    "        if dError <= EPS:\n",
    "            print(\"Converge\", error, dError, count)\n",
    "            break\n",
    "        elif MAX_ITER <= count:\n",
    "            print(\"Not Converge...\", error, dError, count)\n",
    "            break\n",
    "\n",
    "    R = np.array(H[0:-1, 0:-1])\n",
    "    T = np.array(H[0:-1, -1])\n",
    "\n",
    "    return R, T\n",
    "\n",
    "def icp_matching_f(previous_points, current_points):\n",
    "    \"\"\"\n",
    "    Iterative Closest Point matching\n",
    "    - input\n",
    "    previous_points: 2D or 3D points in the previous frame\n",
    "    current_points: 2D or 3D points in the current frame\n",
    "    - output\n",
    "    R: Rotation matrix\n",
    "    T: Translation vector\n",
    "    \"\"\"\n",
    "    H = None  # homogeneous transformation matrix\n",
    "\n",
    "    dError = np.inf\n",
    "    preError = np.inf\n",
    "    count = 0\n",
    "\n",
    "        # Check if the number of points differs\n",
    "    n_points_prev = previous_points.shape[1]\n",
    "    n_points_curr = current_points.shape[1]\n",
    "\n",
    "    # Match points based on the current set's size\n",
    "    if n_points_prev > n_points_curr:\n",
    "        previous_points = previous_points[:, :n_points_curr]  # Down-sample the larger point cloud\n",
    "    elif n_points_curr > n_points_prev:\n",
    "        current_points = current_points[:, :n_points_prev]  # Down-sample the larger point cloud\n",
    "\n",
    "\n",
    "    while dError >= EPS:\n",
    "        count += 1\n",
    "\n",
    "        indexes, error = nearest_neighbor_association(previous_points, current_points)\n",
    "        # Slice based on indexes to align points\n",
    "        previous_matched_points = previous_points[:, indexes]\n",
    "        current_matched_points = current_points\n",
    "\n",
    "        Rt, Tt = svd_motion_estimation_f(previous_matched_points, current_matched_points)\n",
    "\n",
    "        # Apply the transformation to the current points\n",
    "        current_points = (Rt @ current_points) + Tt[:, np.newaxis]\n",
    "\n",
    "        dError = preError - error\n",
    "        if dError < 0:\n",
    "            print(\"Not Converged...\", preError, dError, count)\n",
    "            break\n",
    "\n",
    "        preError = error\n",
    "        H = update_homogeneous_matrix(H, Rt, Tt)\n",
    "\n",
    "        if dError <= EPS:\n",
    "            print(\"Converged\", error, dError, count)\n",
    "            break\n",
    "        elif MAX_ITER <= count:\n",
    "            print(\"Not Converged...\", error, dError, count)\n",
    "            break\n",
    "\n",
    "    R = np.array(H[0:-1, 0:-1])\n",
    "    T = np.array(H[0:-1, -1])\n",
    "\n",
    "    return R, T\n",
    "\n",
    "\n",
    "\n",
    "def svd_motion_estimation_f(previous_points, current_points):\n",
    "    \"\"\"\n",
    "    Estimate the rigid body transformation (rotation and translation) between previous_points and current_points.\n",
    "    Returns:\n",
    "    - R: rotation matrix\n",
    "    - t: translation vector\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    pm = np.mean(previous_points, axis=1)\n",
    "    cm = np.mean(current_points, axis=1)\n",
    "\n",
    "    p_shift = previous_points - pm[:, np.newaxis]\n",
    "    c_shift = current_points - cm[:, np.newaxis]\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    W = c_shift @ p_shift.T\n",
    "    u, s, vh = np.linalg.svd(W)\n",
    "\n",
    "    R = (u @ vh).T\n",
    "    t = pm - (R @ cm)\n",
    "\n",
    "    return R, t\n",
    "\n",
    "\n",
    "def downsample_to_match_size(reference_points, target_points):\n",
    "    \"\"\"\n",
    "    Downsample the larger point cloud so that both have the same number of points.\n",
    "    \n",
    "    Inputs:\n",
    "    - reference_points: np.ndarray of shape (D, N1)\n",
    "    - target_points: np.ndarray of shape (D, N2)\n",
    "    \n",
    "    Returns:\n",
    "    - reference_points_resized: shape (D, N)\n",
    "    - target_points_resized: shape (D, N)\n",
    "    \"\"\"\n",
    "    N1 = reference_points.shape[1]\n",
    "    N2 = target_points.shape[1]\n",
    "    N = min(N1, N2)\n",
    "\n",
    "    if N1 > N:\n",
    "        indices = np.random.choice(N1, N, replace=False)\n",
    "        reference_points = reference_points[:, indices]\n",
    "    if N2 > N:\n",
    "        indices = np.random.choice(N2, N, replace=False)\n",
    "        target_points = target_points[:, indices]\n",
    "\n",
    "    return reference_points, target_points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_homogeneous_matrix(Hin, R, T):\n",
    "\n",
    "    r_size = R.shape[0]\n",
    "    H = np.zeros((r_size + 1, r_size + 1))\n",
    "\n",
    "    H[0:r_size, 0:r_size] = R\n",
    "    H[0:r_size, r_size] = T\n",
    "    H[r_size, r_size] = 1.0\n",
    "\n",
    "    if Hin is None:\n",
    "        return H\n",
    "    else:\n",
    "        return Hin @ H\n",
    "\n",
    "def nearest_neighbor_association_f(previous_points, current_points):\n",
    "    \"\"\"\n",
    "    Associate each point in current_points to its nearest neighbor in previous_points.\n",
    "    Handles differing numbers of points.\n",
    "    Returns:\n",
    "    - indexes: indices of closest previous_points\n",
    "    - error: sum of distances (residual error)\n",
    "    \"\"\"\n",
    "\n",
    "    # KD-Tree for fast NN lookup\n",
    "    tree = cKDTree(previous_points.T)\n",
    "    distances, indexes = tree.query(current_points.T)  # Query the nearest neighbors\n",
    "\n",
    "    # Total ICP residual error\n",
    "    error = np.sum(distances)\n",
    "\n",
    "    return indexes, error\n",
    "\n",
    "\n",
    "def nearest_neighbor_association(previous_points, current_points):\n",
    "\n",
    "    # calc the sum of residual errors\n",
    "    delta_points = previous_points - current_points\n",
    "    d = np.linalg.norm(delta_points, axis=0)\n",
    "    error = sum(d)\n",
    "\n",
    "    # calc index with nearest neighbor assosiation\n",
    "    d = np.linalg.norm(np.repeat(current_points, previous_points.shape[1], axis=1)\n",
    "                       - np.tile(previous_points, (1, current_points.shape[1])), axis=0)\n",
    "    indexes = np.argmin(d.reshape(current_points.shape[1], previous_points.shape[1]), axis=1)\n",
    "\n",
    "    return indexes, error\n",
    "\n",
    "\n",
    "def svd_motion_estimation(previous_points, current_points):\n",
    "    pm = np.mean(previous_points, axis=1)\n",
    "    cm = np.mean(current_points, axis=1)\n",
    "\n",
    "    p_shift = previous_points - pm[:, np.newaxis]\n",
    "    c_shift = current_points - cm[:, np.newaxis]\n",
    "\n",
    "    W = c_shift @ p_shift.T\n",
    "    u, s, vh = np.linalg.svd(W)\n",
    "\n",
    "    R = (u @ vh).T\n",
    "    t = pm - (R @ cm)\n",
    "\n",
    "    return R, t\n",
    "\n",
    "\n",
    "def plot_points(previous_points, current_points, figure):\n",
    "    # for stopping simulation with the esc key.\n",
    "    plt.gcf().canvas.mpl_connect(\n",
    "        'key_release_event',\n",
    "        lambda event: [exit(0) if event.key == 'escape' else None])\n",
    "    if previous_points.shape[0] == 3:\n",
    "        plt.clf()\n",
    "        axes = figure.add_subplot(111, projection='3d')\n",
    "        axes.scatter(previous_points[0, :], previous_points[1, :],\n",
    "                    previous_points[2, :], c=\"r\", marker=\".\")\n",
    "        axes.scatter(current_points[0, :], current_points[1, :],\n",
    "                    current_points[2, :], c=\"b\", marker=\".\")\n",
    "        axes.scatter(0.0, 0.0, 0.0, c=\"r\", marker=\"x\")\n",
    "        figure.canvas.draw()\n",
    "    else:\n",
    "        plt.cla()\n",
    "        plt.plot(previous_points[0, :], previous_points[1, :], \".r\")\n",
    "        plt.plot(current_points[0, :], current_points[1, :], \".b\")\n",
    "        plt.plot(0.0, 0.0, \"xr\")\n",
    "        plt.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                #stacked_points = np.vstack((np.array([p['position'] for p in self.prev_lidar_data])[:,0], np.array([p['position'] for p in self.prev_lidar_data])[:,1])) \n",
    "                #stacked_points_c = np.vstack((np.array([p['position'] for p in self.lidar.relative_sense_data])[:,0], np.array([p['position'] for p in self.lidar.relative_sense_data])[:,1])) \n",
    "                #R, T = icp_matching_f(stacked_points,stacked_points_c)\n",
    "                #self.apply_transformation_to_pose_d(R,T)\n",
    "                #self.plot_matched_cones_by_index(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle and FastSLAM scaffolding based on your LiDAR and car model\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, pose, weight=1.0):\n",
    "        self.pose = np.array(pose)  # [x, y, theta]\n",
    "        self.weight = weight\n",
    "        self.landmarks = {}  # {landmark_id: (mean: np.array([x, y]), covariance: np.array([[2x2]]))}\n",
    "\n",
    "    def predict(self, control_input, dt, motion_noise):\n",
    "        \"\"\"\n",
    "        Apply noisy motion model to update pose\n",
    "        control_input: [delta_x, delta_y, delta_theta]\n",
    "        \"\"\"\n",
    "        noise = np.random.normal(0, motion_noise, 3)\n",
    "        dx, dy, dtheta = control_input + noise\n",
    "        x, y, theta = self.pose\n",
    "        self.pose = np.array([x + dx, y + dy, theta + dtheta])\n",
    "\n",
    "    def update_landmark(self, landmark_id, z_rel, R):\n",
    "        \"\"\"\n",
    "        EKF update of landmark (in world frame) given observation z_rel in car frame\n",
    "        \"\"\"\n",
    "        # Transform relative measurement to global coords\n",
    "        lx = self.pose[0] + np.cos(self.pose[2]) * z_rel[0] - np.sin(self.pose[2]) * z_rel[1]\n",
    "        ly = self.pose[1] + np.sin(self.pose[2]) * z_rel[0] + np.cos(self.pose[2]) * z_rel[1]\n",
    "        z = np.array([lx, ly])\n",
    "\n",
    "        if landmark_id not in self.landmarks:\n",
    "            self.landmarks[landmark_id] = (z, R)\n",
    "            return\n",
    "\n",
    "        mu, sigma = self.landmarks[landmark_id]\n",
    "\n",
    "        # Measurement prediction is just mu\n",
    "        H = np.eye(2)\n",
    "        S = H @ sigma @ H.T + R\n",
    "        K = sigma @ H.T @ np.linalg.inv(S)\n",
    "        mu_new = mu + K @ (z - mu)\n",
    "        sigma_new = (np.eye(2) - K @ H) @ sigma\n",
    "        self.landmarks[landmark_id] = (mu_new, sigma_new)\n",
    "\n",
    "        # Likelihood (used for weight update)\n",
    "        likelihood = multivariate_normal.pdf(z, mean=mu, cov=S)\n",
    "        self.weight *= likelihood\n",
    "\n",
    "\n",
    "def initialize_particles(n, init_pose, noise_std):\n",
    "    return [Particle(pose=init_pose + np.random.normal(0, noise_std, 3)) for _ in range(n)]\n",
    "\n",
    "def resample_particles(particles):\n",
    "    weights = np.array([p.weight for p in particles])\n",
    "    weights /= np.sum(weights)\n",
    "    indices = np.random.choice(len(particles), size=len(particles), p=weights)\n",
    "    resampled = [particles[i] for i in indices]\n",
    "    for p in resampled:\n",
    "        p.weight = 1.0\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mahalanobis_distance(particle, z, lm_id, Q_cov):\n",
    "#     xf = particle.lm[lm_id].reshape(2, 1)\n",
    "#     Pf = particle.lmP[2 * lm_id:2 * lm_id + 2]\n",
    "#     zp, Hv, Hf, Sf = compute_jacobians(particle, xf, Pf, Q_cov)\n",
    "#     dz = z[0:2].reshape(2, 1) - zp\n",
    "#     dz[1, 0] = pi_2_pi(dz[1, 0])\n",
    "#     d2 = float(dz.T @ np.linalg.inv(Sf) @ dz)\n",
    "#     return d2\n",
    "\n",
    "# def update_with_observation(particles, z, mahal_thresh=9.21):\n",
    "#     for iz in range(z.shape[1]):\n",
    "#         for p in particles:\n",
    "#             best_lm_id = None\n",
    "#             best_d2 = float('inf')\n",
    "\n",
    "#             for lm_id in range(p.lm.shape[0]):\n",
    "#                 if np.allclose(p.lm[lm_id], 0):\n",
    "#                     continue\n",
    "\n",
    "#                 # ✴️ New: Euclidean range filter\n",
    "#                 dx = p.lm[lm_id, 0] - p.state.x\n",
    "#                 dy = p.lm[lm_id, 1] - p.state.y\n",
    "#                 dist = math.hypot(dx, dy)\n",
    "#                 if dist > MAX_RANGE + 1.0:  # optional buffer (1.0 m)\n",
    "#                     continue\n",
    "                \n",
    "#                 d2 = compute_mahalanobis_distance(p, z[:, iz], lm_id, Q)\n",
    "#                 if d2 < mahal_thresh and d2 < best_d2:\n",
    "#                     best_d2 = d2\n",
    "#                     best_lm_id = lm_id\n",
    "\n",
    "#             if best_lm_id is not None:\n",
    "#                 z_with_id = np.array([z[0, iz], z[1, iz], best_lm_id])\n",
    "#                 w = compute_weight(p, z_with_id, Q)\n",
    "#                 p.w *= w\n",
    "#                 p = update_landmark(p, z_with_id, Q)\n",
    "#                 p = proposal_sampling(p, z_with_id, Q)\n",
    "#             else:\n",
    "#                 new_id = np.where(np.all(p.lm == 0, axis=1))[0]\n",
    "#                 if len(new_id) == 0:\n",
    "#                     continue\n",
    "#                 z_with_id = np.array([z[0, iz], z[1, iz], new_id[0]])\n",
    "#                 p = add_new_lm(p, z_with_id, Q)\n",
    "#     return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_with_observation(particles, z):\n",
    "#     for iz in range(len(z[0, :])):\n",
    "#         landmark_id = int(z[2, iz])\n",
    "\n",
    "#         for ip in range(N_PARTICLE):\n",
    "#             # new landmark\n",
    "#             if abs(particles[ip].lm[landmark_id, 0]) <= 0.01:\n",
    "#                 particles[ip] = add_new_lm(particles[ip], z[:, iz], Q)\n",
    "#             # known landmark\n",
    "#             else:\n",
    "#                 w = compute_weight(particles[ip], z[:, iz], Q)\n",
    "#                 particles[ip].w *= w\n",
    "\n",
    "#                 particles[ip] = update_landmark(particles[ip], z[:, iz], Q)\n",
    "#                 particles[ip] = proposal_sampling(particles[ip], z[:, iz], Q)\n",
    "\n",
    "#     return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def proposal_sampling(particle, z, Q_cov):\n",
    "#     lm_id = int(z[2])\n",
    "#     xf = particle.lm[lm_id, :].reshape(2, 1)\n",
    "#     Pf = particle.lmP[2 * lm_id:2 * lm_id + 2]\n",
    "#     # State\n",
    "#     x = np.array([particle.state.x, particle.state.y, particle.state.yaw]).reshape(3, 1)\n",
    "#     P = particle.P\n",
    "#     zp, Hv, Hf, Sf = compute_jacobians(particle, xf, Pf, Q_cov)\n",
    "\n",
    "#     Sfi = np.linalg.inv(Sf)\n",
    "#     dz = z[0:2].reshape(2, 1) - zp\n",
    "#     dz[1] = pi_2_pi(dz[1])\n",
    "\n",
    "#     Pi = np.linalg.inv(P + R)\n",
    "\n",
    "#     particle.P = np.linalg.inv(Hv.T @ Sfi @ Hv + Pi)  # proposal covariance\n",
    "#     x += particle.P @ Hv.T @ Sfi @ dz  # proposal mean\n",
    "\n",
    "#     particle.state.x = x[0, 0]\n",
    "#     particle.state.y = x[1, 0]\n",
    "#     particle.state.yaw = x[2, 0]\n",
    "\n",
    "#     return particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_landmark(particle, z, Q_cov):\n",
    "#     lm_id = int(z[2])\n",
    "#     xf = np.array(particle.lm[lm_id]).reshape(2, 1)\n",
    "#     Pf = particle.lmP[lm_id]\n",
    "\n",
    "#     zp, Hv, Hf, Sf = compute_jacobians(particle, xf, Pf, Q_cov)\n",
    "\n",
    "#     dz = z[0:2].reshape(2, 1) - zp\n",
    "#     dz[1, 0] = pi_2_pi(dz[1, 0])\n",
    "\n",
    "#     xf, Pf = update_kf_with_cholesky(xf, Pf, dz, Q, Hf)\n",
    "\n",
    "#     particle.lm[lm_id, :] = xf.T\n",
    "#     particle.lmP[2 * lm_id:2 * lm_id + 2, :] = Pf\n",
    "\n",
    "#     return particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_new_lm(particle, z, Q_cov):\n",
    "#     r = z[0]\n",
    "#     b = z[1]\n",
    "#     lm_id = int(z[2])\n",
    "\n",
    "#     s = math.sin(pi_2_pi(particle.state.yaw + b))\n",
    "#     c = math.cos(pi_2_pi(particle.state.yaw + b))\n",
    "\n",
    "#     particle.lm[lm_id, 0] = particle.state.x + r * c\n",
    "#     particle.lm[lm_id, 1] = particle.state.y + r * s\n",
    "\n",
    "#     # covariance\n",
    "#     dx = r * c\n",
    "#     dy = r * s\n",
    "#     d2 = dx ** 2 + dy ** 2\n",
    "#     d = math.sqrt(d2)\n",
    "#     Gz = np.array([[dx / d, dy / d],\n",
    "#                    [-dy / d2, dx / d2]])\n",
    "#     particle.lmP[2 * lm_id:2 * lm_id + 2] = np.linalg.inv(\n",
    "#         Gz) @ Q_cov @ np.linalg.inv(Gz.T)\n",
    "\n",
    "#     return particle"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
